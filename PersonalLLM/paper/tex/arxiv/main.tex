% -*- Mode: Latex; -*-
% ------------------------------------------------------------------------
% Default style
% ------------------------------------------------------------------------

\documentclass[11pt]{article}
\usepackage[numbers]{natbib}
\usepackage{macros/packages}
\usepackage{macros/editing-macros}
\usepackage{macros/formatting}
\usepackage{macros/statistics-macros}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{%
  calc,
  fit,
  arrows,
  arrows.meta,
  positioning,
  decorations.pathreplacing,
  decorations.shapes,
}

% \onehalfspacing
% \renewcommand{\baselinestretch}{1.35}

\begin{document}

% Control whitespace around equations
\abovedisplayskip=8pt plus0pt minus3pt
\belowdisplayskip=8pt plus0pt minus3pt

% ------------------------------------------------------------------------
% Main Paper Body
% ------------------------------------------------------------------------


% ------------------------------------------------------------------------
% Default title and authorship
% ------------------------------------------------------------------------
\begin{center}
  {\huge PersonalLLM: Tailoring LLMs to Individual Preferences } \\
  \vspace{.5cm} {\Large Thomas P. Zollo$^*$ ~~~ Andrew Siah$^*$}\footnote{* These authors contributed equally to this work}\\
  \vspace{.2cm}
  {\Large ~~~ Naimeng Ye ~~ Ang Li ~~ Hongseok Namkoong} \\
  \vspace{.2cm}
  {\large Columbia University} \\
  \vspace{.2cm}
  \texttt{\{tpz2105, andrew.siah, ny2336, al4263, hongseok.namkoong\}@columbia.edu}
\end{center}
\footnotetext{Published as conference paper in ICLR 2025.}

% ------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------




\begin{comment}
  In-context learning (ICL) has emerged as a powerful learning paradigm. Going
  back to De Finetti’s work on Bayesian inference using observables—as opposed
  to priors on latent factors/parameters—we establish an \emph{explicit}
  equivalence between ICL and Bayesian inference \emph{a la} De Finetti. From
  this view, pre-training is precisely empirical Bayes: it optimizes the
  marginal likelihood of observed sequences; compared to fitting priors in
  conventional empirical Bayes, pre-training fits posterior predictives using
  transformers. Our observation highlights previously under-explored
  capabilities of ICL: statistical inference and uncertainty
  quantification. Our theory highlights the importance of predictive coherence
  and motivates a new regularizer for pre-training sequence models to be
  logically coherent Bayesians statisticians. Our preliminary empirical
  results demonstrate coherency regularization can substantially improve the
  inferential capabilities of ICL.
\end{comment}


\input{sections/00_intro}
\input{sections/01_dataset}
\input{sections/02_analysis}
\input{sections/03_experiments}
\input{sections/04_related_work}
\input{sections/05_discussion&limitations}
\newpage

\section*{Acknowledgments}\label{sec:acknowledgments}

We thank ONR Grant N00014-23-1-2436 for its generous support.  This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R\&E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence). This work was partially funded by the Digital Future Initiative at Columbia Business School.


\bibliographystyle{plainnat}
\bibliography{../cite.bib}

\newpage
\begin{appendix}
\input{sections/appendix}
\end{appendix}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
